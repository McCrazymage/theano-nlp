import argparse
parser = argparse.ArgumentParser(
    description='Train a language model on text file.',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter
)
parser.add_argument(
    'data_file',
    type=str,
    help="Text file for training. Each new line will be treated as a new sequence"
)
parser.add_argument(
    'vocab_file',
    type=str,
    help="Vocabulary file generated by vocab.py."
)
parser.add_argument(
    'output_file',
    type=str,
    help="Output location to write data to."
)

parser.add_argument(
    '--temporary-file',
    type=str,
    default="tmp.model.pkl",
    help="Output location to write temporary model to."
)
parser.add_argument(
    '--max-epochs',
    type=int,
    default=20,
    help="Maximum number of epochs to run through training data."
)
parser.add_argument(
    '--batch-size',
    type=int,
    default=32,
    help="Number of sequences to simultaneously calculate cost."
)
parser.add_argument(
    '--validation-percent',
    type=float,
    default=0.05,
    help="First validation_percent of data will be used as the validation set."
)


parser.add_argument(
    '--checkpoint',
    type=int,
    default=5000,
    help="Saves to temporary_file every time this number of instances are seen."
)

parser.add_argument(
    '--improvement-threshold',
    type=float,
    default=0.95,
    help="Improvement over best cost has to be this much or incur patience."
)
parser.add_argument(
    '--patience',
    type=int,
    default=10,
    help="Number of times improvement_threshold is allowed to be crossed since last best cost."
)


parser.add_argument(
    '--embedding-size',
    type=int,
    default=100,
    help="Size of per character embedding / character representation vector."
)
parser.add_argument(
    '--hidden-size',
    type=int,
    default=100,
    help="Size of hidden LSTM layers."
)
parser.add_argument(
    '--l2',
    type=float,
    default=0.0,
    help="L2 coefficient."
)

args = parser.parse_args()
import sys
import cPickle as pickle
from itertools import islice
from pprint import pprint

import numpy as np
import theano
import theano.tensor as T

from theano_toolkit.parameters import Parameters
from theano_toolkit import updates

import vocab
import model
import data_io

import itertools


def make_batch_train(P, cost, end_id):
    batch = T.imatrix('batch')
    costs, disp_costs = cost(batch, P)
    batch_cost = T.mean(costs)

    print "Calculating gradient..."
    params = P.values()
    grads = T.grad(batch_cost, wrt=params)
    grads_norms = [T.sqrt(T.sum(g**2)) for g in grads]
    deltas = [T.switch(T.gt(n, 5), 5 * g / n, g)
              for n, g in zip(grads_norms, grads)]

    print "Compiling function..."
    _train = theano.function(
        inputs=[batch],
        outputs=T.mean(disp_costs),
        updates=updates.rmsprop(params, deltas)
    )

    def train(batch):
        max_length = max(len(l) for l in batch)
        batch_array = end_id * np.ones((len(batch), max_length), dtype=np.int32)
        for i, l in enumerate(batch):
            batch_array[i, :len(l)] = l
        return _train(batch_array)
    print "Done."
    return train


def make_test(P, cost):
    X = T.imatrix('X')
    _test = theano.function(
        inputs=[X],
        outputs=cost(X, P)[1],
    )

    def test(test_stream):
        total_cost = 0
        total_count = 0
        for seq in test_stream:
            total_cost += _test(np.array(seq, dtype=np.int32).reshape(1, len(seq)))
            total_count += 1
        return total_cost / total_count
    return test

if __name__ == "__main__":
    data_file = args.data_file
    vocab_file = args.vocab_file
    output_file = args.output_file
    tmp_file = args.temporary_file

    max_epochs = args.max_epochs
    batch_size = args.batch_size
    improvement_threshold = args.improvement_threshold
    validation_percent = args.validation_percent
    patience = args.patience
    checkpoint = args.checkpoint

    embedding_size = args.embedding_size
    hidden_size = args.hidden_size

    l2_coefficient = args.l2

    id2char = pickle.load(open(vocab_file,'r'))
    char2id = vocab.load(vocab_file)
    P = Parameters()
    lang_model = model.build(P,
                             character_count=len(char2id) + 1,
                             embedding_size=embedding_size,
                             hidden_size=hidden_size
                             )

    def cost(X, P): # batch_size x time
        eps = 1e-3
        X = X.T                                         # time x batch_size
        char_prob_dist = lang_model(X[:-1])                # time x batch_size x output_size
        char_prob_dist = (1 - 2 * eps) * char_prob_dist + eps
        label_prob = char_prob_dist[
            T.arange(X.shape[0] - 1).dimshuffle(0, 'x'),
            T.arange(X.shape[1]).dimshuffle('x', 0),
            X[1:]
        ]                                                # time x batch_size
        cross_entropy = -T.sum(T.log(label_prob), axis=0)
        display_cost = 2**(-T.mean(T.log2(label_prob), axis=0))
        l2 = sum(T.sum(p**2) for p in P.values())
        cost = cross_entropy
        if l2_coefficient > 0:
            cost += l2_coefficient * l2
        return cost, display_cost

    params = P.values()
    train = make_batch_train(P, cost, end_id=char2id["\n"])
    test = make_test(P, cost)

    # Count number of items in the dataset
    line_count = sum(1 for _ in open(data_file,'r'))
    validation_count = int(round(line_count * validation_percent))
    print "Total data:",line_count, "Validation set:", validation_count

    # Training process

    best_cost = np.inf
    increase_count = 0
    seen = 0
    for epoch in xrange(max_epochs):
        print "Epoch:", epoch + 1
        print "Batch size:", batch_size

        # Run test on validation set
        data_stream = data_io.stream(data_file, char2id)
        test_stream = islice(data_stream, validation_count)
        test_cost = test(test_stream)
        print "Perplexity:", test_cost

        if test_cost < improvement_threshold * best_cost:
            best_cost = test_cost
            P.save(output_file)
            increase_count = 0
        else:
            increase_count += 1
            if increase_count > patience:
                break

        # Run training
        data_stream = data_io.randomise(data_stream, buffer_size=1024)
        data_stream = data_io.sortify(data_stream, key=lambda x: len(x), buffer_size=512)
        batch_data_stream = data_io.batch(data_stream, batch_size=batch_size)
        batch_data_stream = data_io.randomise(batch_data_stream)

        for batch in batch_data_stream:
            avg_cost = train(batch)
            if np.isnan(avg_cost):
                pprint([''.join(id2char[c] for c in l[1:]) for l in batch])
                exit(1)
            print avg_cost,
            seen += len(batch)
            if seen > checkpoint:
                print "Saving...",
                P.save(tmp_file)
                seen = 0

            print
            
    print "Best cost was", best_cost, "model saved at", output_file
